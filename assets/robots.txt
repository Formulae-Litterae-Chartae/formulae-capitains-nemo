# robots.txt
## Although the "*" should include all bots. I did observe, that certain ones do need specific instructions.
## The goal is to disallow the crawling and automatic processing of all sites with a "+" or "%2B" (equivalent in percentage encoding)
## Since the site is refreshed quite infrequent, we used a Crawl-delay of 30 seconds
User-agent: *
User-agent: AdsBot-Google
User-agent: Yandex
User-agent: YandexBot
User-agent: AcademicBotRTU
User-agent: Googlebot
Disallow: *+*
Disallow: *%2B*
Crawl-delay: 30